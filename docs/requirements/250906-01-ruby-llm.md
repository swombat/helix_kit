# Ruby LLM Integration

We're going to build the integration of this app with https://rubyllm.com/. Fetch the documentation from https://rubyllm.com relevant for this integration. In particular, be sure to fetch and detail the rails integration instructions at https://rubyllm.com/rails/.

The goal of this work is to have a clean integration of RubyLLM with suitable model objects in the app, with a goal to enable easy creation of "chat/conversation" style features, and, later, "agentic conversation" style features. We are only looking at the Rails side of things right now, not the Svelte side, but eventually the intention is to have a clean integration on both sides, so bear the information in `docs/synchronization-usage.md` and `docs/stack/inertia-rails.md` in mind as you design this spec.

We want to have a clean way to store conversations under accounts, with multiple users being able to view and take part in a conversation (so the conversations really are attached to the account, not to the user).

Consider also the possibility of uploading/attaching documents into a conversation, mid-conversation. This ought to be done in a similar way as other uploads (like the avatar feature), using S3 to store the documents and then pass them to RubyLLM appropriately.

Updates to Rails message object should be streaming so that the UI can be updated live (but debounced so as not to swamp the browser with too many updates).

In spec:
- Image responses (see https://rubyllm.com/image-generation/)
- Audio responses (see https://rubyllm.com/chat/#multi-modal-conversations)

Out of scope for this iteration but coming later (with the agentic conversation features):
- Tool calling
- MCP servers

Out of scope for the long run:
- Embeddings
- RAG
