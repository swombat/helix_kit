# Ruby LLM Integration - Final Specification

## Executive Summary

A dead-simple Rails implementation for AI conversations using the ruby-openai gem. Two database tables, fat models with AI logic, thin RESTful controllers, and direct ActionCable streaming. No premature optimization, no unnecessary complexity, just boring Rails code that works.

## Core Requirements

1. **Multi-user conversations** - Multiple users from the same account can participate
2. **Conversation history** - Full history stored and accessible
3. **Model selection** - Each conversation picks its LLM model
4. **File attachments** - Support documents, images, and audio via Active Storage
5. **Real-time streaming** - Stream AI responses via ActionCable

## Database Design (2 Tables)

### conversations table
```ruby
create_table :conversations do |t|
  t.references :account, null: false, foreign_key: true
  t.string :title
  t.string :model, null: false, default: "gpt-4o"
  t.timestamps
end

add_index :conversations, [:account_id, :created_at]
```

### messages table
```ruby
create_table :messages do |t|
  t.references :conversation, null: false, foreign_key: true
  t.references :user, foreign_key: true # null for AI messages
  t.string :role, null: false # "user", "assistant", "system"
  t.text :content
  t.timestamps
end

add_index :messages, [:conversation_id, :created_at]
```

## Models

### app/models/conversation.rb
```ruby
class Conversation < ApplicationRecord
  belongs_to :account
  has_many :messages, dependent: :destroy
  has_many :participants, -> { distinct }, through: :messages, source: :user
  
  validates :model, presence: true, inclusion: { 
    in: Rails.configuration.ai_models.keys 
  }
  
  # Use AI to generate title after first exchange
  after_create :schedule_title_generation
  
  def send_message(user:, content:, attachments: [])
    message = messages.create!(
      user: user,
      role: "user",
      content: content
    )
    
    # Attach files if provided
    attachments.each { |file| message.files.attach(file) }
    
    # Generate AI response
    generate_ai_response(message)
  end
  
  def generate_ai_response(user_message)
    # Build conversation context
    context = messages.order(:created_at).map do |msg|
      { role: msg.role, content: msg.content_with_attachments }
    end
    
    # Create placeholder for streaming
    ai_message = messages.create!(
      role: "assistant",
      content: ""
    )
    
    # Stream response in background job for reliability
    StreamAiResponseJob.perform_later(self, ai_message, context)
    
    ai_message
  end
  
  private
  
  def schedule_title_generation
    GenerateConversationTitleJob.perform_later(self) unless title?
  end
end
```

### app/models/message.rb
```ruby
class Message < ApplicationRecord
  belongs_to :conversation
  belongs_to :user, optional: true # AI messages have no user
  
  has_many_attached :files
  
  validates :role, presence: true, inclusion: { in: %w[user assistant system] }
  validates :content, presence: true
  validate :user_required_for_user_role
  
  scope :from_users, -> { where(role: "user") }
  scope :from_ai, -> { where(role: "assistant") }
  
  # Simple attachment listing for AI context
  def content_with_attachments
    return content unless files.attached?
    
    filenames = files.map(&:filename).join(", ")
    "#{content}\n\n[Attached: #{filenames}]"
  end
  
  private
  
  def user_required_for_user_role
    if role == "user" && user_id.blank?
      errors.add(:user, "is required for user messages")
    elsif role != "user" && user_id.present?
      errors.add(:user, "must be blank for AI messages")
    end
  end
end
```

## Background Jobs

### app/jobs/stream_ai_response_job.rb
```ruby
class StreamAiResponseJob < ApplicationJob
  class AiServiceError < StandardError; end
  class RateLimitError < AiServiceError; end
  class ApiKeyError < AiServiceError; end
  
  retry_on RateLimitError, wait: :polynomially_longer, attempts: 3
  discard_on ApiKeyError
  
  def perform(conversation, ai_message, context)
    client = ai_client_for(conversation.model)
    buffer = ""
    
    client.chat(
      parameters: {
        model: conversation.model,
        messages: context,
        stream: streaming_handler_for(conversation, ai_message, buffer)
      }
    )
    
    # Save final content
    ai_message.update!(content: buffer)
    
    # Broadcast completion
    ActionCable.server.broadcast(
      "conversation_#{conversation.id}",
      { type: "complete", message_id: ai_message.id }
    )
    
  rescue OpenAI::RateLimitError => e
    handle_rate_limit(e)
  rescue OpenAI::AuthenticationError => e
    handle_auth_error(e)
  rescue OpenAI::Error => e
    handle_api_error(ai_message, e)
  rescue StandardError => e
    handle_unexpected_error(ai_message, e)
  end
  
  private
  
  def ai_client_for(model)
    config = Rails.configuration.ai_models[model]
    
    case config[:provider]
    when :openai
      OpenAI::Client.new(
        access_token: Rails.application.credentials.openai_api_key
      )
    when :anthropic
      # Add when needed
      raise NotImplementedError, "Anthropic support coming soon"
    else
      raise ArgumentError, "Unknown provider: #{config[:provider]}"
    end
  end
  
  def streaming_handler_for(conversation, message, buffer)
    proc do |chunk|
      if text = chunk.dig("choices", 0, "delta", "content")
        buffer << text
        
        # Periodic saves to prevent data loss
        message.update_column(:content, buffer) if buffer.length % 500 == 0
        
        # Broadcast chunk
        ActionCable.server.broadcast(
          "conversation_#{conversation.id}",
          {
            type: "chunk",
            message_id: message.id,
            content: text
          }
        )
      end
    end
  end
  
  def handle_rate_limit(error)
    Rails.logger.warn("Rate limit hit: #{error.message}")
    raise RateLimitError, error.message
  end
  
  def handle_auth_error(error)
    Rails.logger.error("API authentication failed: #{error.message}")
    Honeybadger.notify(error)
    raise ApiKeyError, error.message
  end
  
  def handle_api_error(message, error)
    Rails.logger.error("AI API Error: #{error.class} - #{error.message}")
    
    message.update!(
      content: "I encountered an error processing your request. Please try again."
    )
    
    ActionCable.server.broadcast(
      "conversation_#{message.conversation_id}",
      {
        type: "error",
        message_id: message.id,
        error: "Service temporarily unavailable"
      }
    )
  end
  
  def handle_unexpected_error(message, error)
    Rails.logger.error("Unexpected error: #{error.class} - #{error.message}")
    Rails.logger.error(error.backtrace.join("\n"))
    Honeybadger.notify(error)
    
    message.update!(
      content: "An unexpected error occurred. Our team has been notified."
    )
  end
end
```

### app/jobs/generate_conversation_title_job.rb
```ruby
class GenerateConversationTitleJob < ApplicationJob
  def perform(conversation)
    return if conversation.title.present?
    return unless conversation.messages.count >= 2 # Wait for AI response
    
    # Build a simple prompt for title generation
    first_exchange = conversation.messages
                                .order(:created_at)
                                .limit(2)
                                .pluck(:content)
                                .join("\n\n")
    
    client = OpenAI::Client.new(
      access_token: Rails.application.credentials.openai_api_key
    )
    
    response = client.chat(
      parameters: {
        model: "gpt-4o-mini", # Use cheap model for titles
        messages: [
          {
            role: "system",
            content: "Generate a concise title (max 6 words) for this conversation. Return only the title, no quotes or punctuation."
          },
          {
            role: "user",
            content: first_exchange
          }
        ],
        max_tokens: 20
      }
    )
    
    title = response.dig("choices", 0, "message", "content")&.strip
    conversation.update!(title: title) if title.present?
    
  rescue => e
    # Title generation is non-critical
    Rails.logger.info("Title generation failed: #{e.message}")
  end
end
```

## Controllers

### app/controllers/conversations_controller.rb
```ruby
class ConversationsController < ApplicationController
  before_action :set_conversation, only: [:show, :destroy]
  
  def index
    @conversations = current_account.conversations
                                   .includes(:messages)
                                   .order(updated_at: :desc)
    
    render inertia: "Conversations/Index", props: {
      conversations: @conversations.map { |c| conversation_json(c) }
    }
  end
  
  def show
    @messages = @conversation.messages
                            .includes(:user, files_attachments: :blob)
                            .order(:created_at)
    
    render inertia: "Conversations/Show", props: {
      conversation: conversation_json(@conversation),
      messages: @messages.map { |m| message_json(m) },
      available_models: Rails.configuration.ai_models.keys
    }
  end
  
  def create
    @conversation = current_account.conversations.create!(conversation_params)
    redirect_to @conversation
  end
  
  def destroy
    @conversation.destroy
    redirect_to conversations_path
  end
  
  private
  
  def set_conversation
    @conversation = current_account.conversations.find(params[:id])
  end
  
  def conversation_params
    params.require(:conversation).permit(:title, :model)
  end
  
  def conversation_json(conversation)
    {
      id: conversation.id,
      title: conversation.title || "New Conversation",
      model: conversation.model,
      last_message_at: conversation.messages.maximum(:created_at),
      message_count: conversation.messages.count
    }
  end
  
  def message_json(message)
    {
      id: message.id,
      role: message.role,
      content: message.content,
      user: message.user && {
        id: message.user.id,
        name: message.user.full_name,
        avatar_url: message.user.avatar_url
      },
      files: message.files.map { |f| 
        {
          id: f.id,
          filename: f.filename.to_s,
          url: rails_blob_url(f)
        }
      },
      created_at: message.created_at
    }
  end
end
```

### app/controllers/messages_controller.rb
```ruby
class MessagesController < ApplicationController
  before_action :set_conversation
  
  def create
    @user_message = @conversation.send_message(
      user: current_user,
      content: params[:content],
      attachments: params[:files] || []
    )
    
    # Return both messages: user and AI placeholder
    render json: {
      user_message: message_json(@user_message),
      ai_message: message_json(@conversation.messages.last)
    }
  end
  
  private
  
  def set_conversation
    @conversation = current_account.conversations.find(params[:conversation_id])
  end
  
  def message_json(message)
    {
      id: message.id,
      role: message.role,
      content: message.content,
      user: message.user && {
        id: message.user.id,
        name: message.user.full_name
      },
      created_at: message.created_at
    }
  end
end
```

## ActionCable Channel

### app/channels/conversation_channel.rb
```ruby
class ConversationChannel < ApplicationCable::Channel
  def subscribed
    conversation = current_user.accounts
                              .joins(:conversations)
                              .find_by(conversations: { id: params[:id] })
    
    if conversation
      stream_from "conversation_#{params[:id]}"
    else
      reject
    end
  end
end
```

## Frontend Components

### app/frontend/pages/Conversations/Show.svelte
```svelte
<script>
  import { onMount, onDestroy } from 'svelte';
  import { page } from '@inertiajs/svelte';
  import MessageList from '../../components/MessageList.svelte';
  import MessageInput from '../../components/MessageInput.svelte';
  
  let { conversation, messages: initialMessages, available_models } = $props();
  let messages = $state(initialMessages);
  let cable = $state(null);
  let subscription = $state(null);
  let isStreaming = $state(false);
  
  onMount(() => {
    // Setup ActionCable subscription
    cable = window.App.cable;
    subscription = cable.subscriptions.create(
      { 
        channel: "ConversationChannel", 
        id: conversation.id 
      },
      {
        received(data) {
          handleCableMessage(data);
        }
      }
    );
  });
  
  onDestroy(() => {
    if (subscription) {
      subscription.unsubscribe();
    }
  });
  
  function handleCableMessage(data) {
    switch(data.type) {
      case 'chunk':
        // Update message content as it streams
        const message = messages.find(m => m.id === data.message_id);
        if (message) {
          message.content += data.content;
        }
        break;
        
      case 'complete':
        isStreaming = false;
        break;
        
      case 'error':
        isStreaming = false;
        console.error('AI Error:', data.error);
        // Could show toast notification here
        break;
    }
  }
  
  async function sendMessage(content, files) {
    const formData = new FormData();
    formData.append('content', content);
    files.forEach(file => formData.append('files[]', file));
    
    isStreaming = true;
    
    const response = await fetch(`/conversations/${conversation.id}/messages`, {
      method: 'POST',
      headers: {
        'X-CSRF-Token': document.querySelector('[name="csrf-token"]').content
      },
      body: formData
    });
    
    const { user_message, ai_message } = await response.json();
    
    // Add both messages to the list
    messages = [...messages, user_message, ai_message];
  }
</script>

<div class="flex flex-col h-screen">
  <header class="p-4 border-b">
    <h1 class="text-xl font-semibold">
      {conversation.title || 'New Conversation'}
    </h1>
    <div class="text-sm text-gray-500">
      Model: {conversation.model}
    </div>
  </header>
  
  <MessageList {messages} />
  
  <MessageInput 
    onSend={sendMessage} 
    disabled={isStreaming}
  />
</div>
```

## Configuration

### config/ai_models.yml
```yaml
# AI model configuration
# Prices are per 1K tokens
development:
  gpt-4o:
    provider: openai
    input_price: 0.0025
    output_price: 0.01
  gpt-4o-mini:
    provider: openai
    input_price: 0.00015
    output_price: 0.0006
  claude-3-5-sonnet:
    provider: anthropic
    input_price: 0.003
    output_price: 0.015

production:
  gpt-4o:
    provider: openai
    input_price: 0.0025
    output_price: 0.01
  gpt-4o-mini:
    provider: openai
    input_price: 0.00015
    output_price: 0.0006
```

### config/application.rb
```ruby
# Load AI model configuration
config.ai_models = config_for(:ai_models)
```

## Routes

```ruby
# config/routes.rb
resources :conversations do
  resources :messages, only: [:create]
end
```

## Migration

```ruby
class CreateConversationsAndMessages < ActiveRecord::Migration[7.1]
  def change
    create_table :conversations do |t|
      t.references :account, null: false, foreign_key: true
      t.string :title
      t.string :model, null: false, default: "gpt-4o"
      t.timestamps
    end
    
    add_index :conversations, [:account_id, :created_at]
    
    create_table :messages do |t|
      t.references :conversation, null: false, foreign_key: true
      t.references :user, foreign_key: true
      t.string :role, null: false
      t.text :content
      t.timestamps
    end
    
    add_index :messages, [:conversation_id, :created_at]
  end
end
```

## Implementation Checklist

### Phase 1: Database and Models
- [ ] Create migration for conversations and messages tables
- [ ] Create config/ai_models.yml
- [ ] Implement Conversation model
- [ ] Implement Message model
- [ ] Add OpenAI credentials to Rails credentials

### Phase 2: Background Jobs
- [ ] Create StreamAiResponseJob with proper error handling
- [ ] Create GenerateConversationTitleJob
- [ ] Test job retry logic with rate limits

### Phase 3: Controllers and Routes
- [ ] Create ConversationsController
- [ ] Create MessagesController
- [ ] Add routes for conversations and messages
- [ ] Test basic CRUD operations

### Phase 4: ActionCable
- [ ] Create ConversationChannel
- [ ] Test streaming with OpenAI API
- [ ] Verify authorization works correctly

### Phase 5: Frontend
- [ ] Create Conversation index page
- [ ] Create Conversation show page
- [ ] Implement MessageList component
- [ ] Implement MessageInput with file upload
- [ ] Add streaming indicators

### Phase 6: Testing
- [ ] Write model tests for Conversation
- [ ] Write model tests for Message
- [ ] Write controller tests
- [ ] Add system tests for full chat flow
- [ ] Test error scenarios

## Key Design Decisions

1. **Background jobs for streaming** - More reliable than in-request streaming, handles failures gracefully
2. **No token/cost tracking** - Add later if needed for billing, not before
3. **Configuration in YAML** - Models and pricing belong in config, not code
4. **Proper error classes** - Specific exceptions for different failure modes
5. **Simple attachment handling** - Just list filenames, no mime type detection
6. **AI-generated titles** - Done properly with AI or not at all
7. **Server-created AI messages** - No fake records in frontend

## What This Doesn't Include

- Token counting (add when billing requires it)
- Cost tracking (premature optimization)
- Model versioning per message (YAGNI)
- Complex attachment processing (KISS)
- Conversation branching (not in requirements)
- System prompts UI (not in requirements)

## Notes

- The `ruby-openai` gem is already in the Gemfile
- Active Storage is configured for file attachments
- ActionCable uses existing session-based authentication
- Honeybadger (or similar) for error tracking
- Background jobs use Solid Queue (Rails 8 default)

This implementation is ~250 lines of application code. It's boring, predictable, and maintainable. Any Rails developer can understand it. That's the point.