# RubyLLM Integration - Technical Specification

## Executive Summary

Integration of the RubyLLM gem to enable AI-powered chat conversations within accounts. This specification uses RubyLLM's Rails integration features (`acts_as_chat`, `acts_as_message`) and integrates with the existing ActionCable synchronization system via the Broadcastable concern. Multiple users within an account can participate in conversations, with support for file attachments, streaming responses, and multi-modal content (images/audio).

## Architecture Overview

### Core Components
- **RubyLLM Gem**: Provides unified API for 500+ AI models via OpenRouter and direct providers
- **Chat Model**: Uses `acts_as_chat` for conversation management  
- **Message Model**: Uses `acts_as_message` for message persistence
- **Broadcastable Concern**: Existing synchronization system for real-time updates
- **ActiveStorage**: File attachments for documents, images, and audio
- **Background Jobs**: Streaming AI responses via Solid Queue

### Data Flow
1. User sends message through controller
2. Message saved to database with user association
3. Background job initiated for AI response
4. RubyLLM streams response chunks
5. Chunks broadcast via existing ActionCable system using Broadcastable
6. Frontend receives updates through existing sync subscriptions

## Step-by-Step Implementation

### Phase 1: RubyLLM Setup

- [ ] Add `ruby_llm` gem to Gemfile
  ```ruby
  gem 'ruby_llm', '~> 1.7'
  ```

- [ ] Bundle install
  ```bash
  bundle install
  ```

- [ ] Run RubyLLM generator
  ```bash
  rails generate ruby_llm:install
  ```

- [ ] Run migrations created by generator
  ```bash
  rails db:migrate
  ```

- [ ] Populate model registry
  ```bash
  rails ruby_llm:load_models
  ```

- [ ] Configure API keys in credentials
  ```bash
  rails credentials:edit
  ```
  ```yaml
  openrouter:
    api_key: your_key_here
  openai:
    api_key: your_key_here  # optional, for direct OpenAI
  anthropic:
    api_key: your_key_here  # optional, for direct Anthropic
  ```

- [ ] Create initializer `config/initializers/ruby_llm.rb`
  ```ruby
  RubyLLM.configure do |config|
    config.openrouter_api_key = Rails.application.credentials.dig(:openrouter, :api_key)
    config.openai_api_key = Rails.application.credentials.dig(:openai, :api_key)
    config.anthropic_api_key = Rails.application.credentials.dig(:anthropic, :api_key)
  end
  ```

### Phase 2: Model Implementation

- [ ] Enhance Chat model with RubyLLM and broadcasting
  ```ruby
  # app/models/chat.rb
  class Chat < ApplicationRecord
    include SyncAuthorizable
    include Broadcastable
    
    # RubyLLM integration
    acts_as_chat
    
    # Associations
    belongs_to :account
    has_many :messages, dependent: :destroy
    has_many :participants, -> { distinct }, through: :messages, source: :user
    
    # Broadcasting configuration
    broadcasts_to :account  # Broadcast changes to account channel
    broadcasts_refresh_prop :chat, collection: true
    skip_broadcasts_on_destroy :messages
    
    # Validations
    validates :model_id, presence: true
    
    # Callbacks
    after_create :generate_title_later, unless: :title?
    
    # Business logic
    def send_user_message(user:, content:, files: [])
      message = messages.build(
        user: user,
        role: 'user',
        content: content
      )
      
      files.each { |file| message.files.attach(file) } if files.present?
      message.save!
      
      # Queue AI response generation
      AiResponseJob.perform_later(self, message)
      message
    end
    
    def generate_title
      return if title.present? || messages.count < 2
      
      first_exchange = messages.order(:created_at).limit(2).pluck(:content).join("\n\n")
      
      title_chat = RubyLLM.chat(model: 'openrouter/auto')
      response = title_chat.ask(
        "Generate a concise title (max 6 words) for this conversation. Return only the title, no quotes or punctuation.\n\nConversation:\n#{first_exchange}",
        max_tokens: 20
      )
      
      update!(title: response.content.strip)
    rescue => e
      Rails.logger.warn "Title generation failed: #{e.message}"
    end
    
    private
    
    def generate_title_later
      GenerateTitleJob.perform_later(self)
    end
  end
  ```

- [ ] Enhance Message model with RubyLLM
  ```ruby
  # app/models/message.rb
  class Message < ApplicationRecord
    include Broadcastable
    
    # RubyLLM integration
    acts_as_message
    
    # Associations
    belongs_to :chat, touch: true
    belongs_to :user, optional: true  # nil for AI messages
    
    # File attachments via ActiveStorage (RubyLLM handles these)
    has_many_attached :files
    
    # Broadcasting - notify chat channel when message changes
    broadcasts_to :chat
    broadcasts_refresh_prop :message
    
    # Validations
    validates :role, presence: true, inclusion: { in: %w[user assistant system] }
    validates :content, presence: true
    validate :user_required_for_user_role
    
    # Scopes
    scope :from_users, -> { where(role: 'user') }
    scope :from_ai, -> { where(role: 'assistant') }
    scope :ordered, -> { order(:created_at) }
    
    # Stream content updates with debouncing
    def append_content!(chunk)
      @buffer ||= content || ""
      @buffer += chunk
      @last_update ||= Time.current
      
      # Debounce updates - save every 100 chars or 0.5 seconds
      if @buffer.length - (content || "").length >= 100 || 
         Time.current - @last_update >= 0.5
        update_column(:content, @buffer)
        @last_update = Time.current
        broadcast_refresh  # Trigger sync update
      end
    end
    
    # Finalize streaming
    def finalize_content!
      update!(content: @buffer) if @buffer
      broadcast_refresh
    end
    
    private
    
    def user_required_for_user_role
      if role == 'user' && user_id.blank?
        errors.add(:user, "is required for user messages")
      elsif role != 'user' && user_id.present?
        errors.add(:user, "must be blank for AI messages")
      end
    end
  end
  ```

- [ ] Add ToolCall model (required by RubyLLM)
  ```ruby
  # app/models/tool_call.rb
  class ToolCall < ApplicationRecord
    acts_as_tool_call
    
    # Note: We're not using tools yet, but RubyLLM requires this model
  end
  ```

- [ ] Add Model metadata model
  ```ruby
  # app/models/llm_model.rb (avoid name collision with Model)
  class LlmModel < ApplicationRecord
    self.table_name = 'models'  # Use RubyLLM's table
    
    acts_as_model
    
    # Scopes for model selection
    scope :available, -> { where(enabled: true) }
    scope :by_provider, ->(provider) { where(provider: provider) }
    scope :multimodal, -> { where(supports_images: true) }
    scope :supports_audio, -> { where(supports_audio: true) }
  end
  ```

### Phase 3: Background Jobs

- [ ] Create AI response streaming job
  ```ruby
  # app/jobs/ai_response_job.rb
  class AiResponseJob < ApplicationJob
    queue_as :ai_processing
    
    class AiServiceError < StandardError; end
    class RateLimitError < AiServiceError; end
    
    retry_on RateLimitError, wait: :polynomially_longer, attempts: 3
    retry_on RubyLLM::RateLimitError, wait: :polynomially_longer, attempts: 3
    
    def perform(chat, user_message)
      # Create AI message placeholder
      ai_message = chat.messages.create!(
        role: 'assistant',
        content: ''
      )
      
      begin
        # Stream response using RubyLLM
        chat.ask(user_message.content) do |chunk|
          # Append content with debouncing
          ai_message.append_content!(chunk.content) if chunk.content
        end
        
        # Finalize the message
        ai_message.finalize_content!
        
      rescue RubyLLM::RateLimitError => e
        Rails.logger.warn "Rate limit hit: #{e.message}"
        raise RateLimitError, e.message
        
      rescue RubyLLM::APIError => e
        handle_api_error(ai_message, e)
        
      rescue StandardError => e
        handle_unexpected_error(ai_message, e)
      end
    end
    
    private
    
    def handle_api_error(message, error)
      Rails.logger.error "AI API Error: #{error.class} - #{error.message}"
      
      message.update!(
        content: "I encountered an error processing your request. Please try again."
      )
      
      Honeybadger.notify(error) if defined?(Honeybadger)
    end
    
    def handle_unexpected_error(message, error)
      Rails.logger.error "Unexpected error in AI response: #{error.class} - #{error.message}"
      Rails.logger.error error.backtrace.join("\n")
      
      message.update!(
        content: "An unexpected error occurred. Our team has been notified."
      )
      
      Honeybadger.notify(error) if defined?(Honeybadger)
    end
  end
  ```

- [ ] Create title generation job
  ```ruby
  # app/jobs/generate_title_job.rb
  class GenerateTitleJob < ApplicationJob
    queue_as :low_priority
    
    def perform(chat)
      chat.generate_title
    end
  end
  ```

### Phase 4: Controllers

- [ ] Create Chats controller
  ```ruby
  # app/controllers/chats_controller.rb
  class ChatsController < ApplicationController
    before_action :set_chat, only: [:show, :update, :destroy]
    
    def index
      @chats = current_account.chats
                             .includes(:messages)
                             .order(updated_at: :desc)
      
      render inertia: 'Chats/Index', props: {
        chats: @chats.map { |c| chat_json(c) }
      }
    end
    
    def show
      @messages = @chat.messages
                      .includes(:user, files_attachments: :blob)
                      .ordered
      
      render inertia: 'Chats/Show', props: {
        chat: chat_json(@chat),
        messages: @messages.map { |m| message_json(m) },
        available_models: available_models
      }
    end
    
    def create
      @chat = current_account.chats.build(chat_params)
      @chat.model_id ||= default_model_id
      
      if @chat.save
        redirect_to account_chat_path(current_account, @chat)
      else
        redirect_back fallback_location: account_chats_path(current_account),
                     inertia: { errors: @chat.errors }
      end
    end
    
    def update
      if @chat.update(chat_params)
        redirect_to account_chat_path(current_account, @chat)
      else
        redirect_back fallback_location: account_chat_path(current_account, @chat),
                     inertia: { errors: @chat.errors }
      end
    end
    
    def destroy
      @chat.destroy
      redirect_to account_chats_path(current_account)
    end
    
    private
    
    def set_chat
      @chat = current_account.chats.find(params[:id])
    end
    
    def chat_params
      params.require(:chat).permit(:title, :model_id)
    end
    
    def chat_json(chat)
      {
        id: chat.id,
        title: chat.title || 'New Conversation',
        model_id: chat.model_id,
        model_name: chat.model&.name,
        last_message_at: chat.messages.maximum(:created_at),
        message_count: chat.messages.count,
        created_at: chat.created_at,
        updated_at: chat.updated_at
      }
    end
    
    def message_json(message)
      {
        id: message.id,
        chat_id: message.chat_id,
        role: message.role,
        content: message.content,
        user: message.user && {
          id: message.user.id,
          name: message.user.full_name,
          avatar_url: message.user.avatar_url,
          initials: message.user.initials
        },
        files: message.files.map { |file|
          {
            id: file.id,
            filename: file.filename.to_s,
            byte_size: file.byte_size,
            content_type: file.content_type,
            url: rails_blob_url(file, only_path: true)
          }
        },
        created_at: message.created_at,
        updated_at: message.updated_at
      }
    end
    
    def available_models
      LlmModel.available.pluck(:id, :name, :provider).map do |id, name, provider|
        { id: id, name: name, provider: provider }
      end
    end
    
    def default_model_id
      'openrouter/auto'  # Let OpenRouter choose the best model
    end
  end
  ```

- [ ] Create Messages controller
  ```ruby
  # app/controllers/messages_controller.rb
  class MessagesController < ApplicationController
    before_action :set_chat
    
    def create
      @message = @chat.send_user_message(
        user: current_user,
        content: message_params[:content],
        files: message_params[:files]
      )
      
      if @message.persisted?
        # Return the user message immediately
        # AI response will stream via ActionCable
        render json: {
          message: message_json(@message),
          status: 'processing'
        }
      else
        render json: { 
          errors: @message.errors 
        }, status: :unprocessable_entity
      end
    end
    
    private
    
    def set_chat
      @chat = current_account.chats.find(params[:chat_id])
    end
    
    def message_params
      params.require(:message).permit(:content, files: [])
    end
    
    def message_json(message)
      {
        id: message.id,
        chat_id: message.chat_id,
        role: message.role,
        content: message.content,
        user: message.user && {
          id: message.user.id,
          name: message.user.full_name,
          avatar_url: message.user.avatar_url
        },
        files: message.files.map { |file|
          {
            id: file.id,
            filename: file.filename.to_s,
            url: rails_blob_url(file, only_path: true)
          }
        },
        created_at: message.created_at
      }
    end
  end
  ```

### Phase 5: Routing

- [ ] Add routes for chats and messages
  ```ruby
  # config/routes.rb
  Rails.application.routes.draw do
    # ... existing routes ...
    
    resources :accounts do
      resources :chats do
        resources :messages, only: [:create]
      end
    end
    
    # ... rest of routes ...
  end
  ```

### Phase 6: Image Generation Support

- [ ] Add image generation capability to Chat model
  ```ruby
  # app/models/chat.rb (addition)
  def generate_image(prompt:, user:)
    # Create user message for the prompt
    user_message = messages.create!(
      user: user,
      role: 'user',
      content: "Generate an image: #{prompt}"
    )
    
    # Generate image in background
    GenerateImageJob.perform_later(self, user_message, prompt)
    user_message
  end
  ```

- [ ] Create image generation job
  ```ruby
  # app/jobs/generate_image_job.rb
  class GenerateImageJob < ApplicationJob
    queue_as :ai_processing
    
    def perform(chat, user_message, prompt)
      # Create AI message for response
      ai_message = chat.messages.create!(
        role: 'assistant',
        content: 'Generating image...'
      )
      
      begin
        # Generate image using RubyLLM
        image_url = RubyLLM.paint(
          prompt,
          model: 'dall-e-3',
          size: '1024x1024'
        )
        
        # Download and attach image
        require 'open-uri'
        image_data = URI.open(image_url)
        
        ai_message.files.attach(
          io: image_data,
          filename: "generated_#{Time.current.to_i}.png",
          content_type: 'image/png'
        )
        
        ai_message.update!(
          content: "Here's the generated image for: #{prompt}"
        )
        
      rescue RubyLLM::ContentPolicyError => e
        ai_message.update!(
          content: "Image generation was blocked: #{e.message}"
        )
        
      rescue => e
        Rails.logger.error "Image generation failed: #{e.message}"
        ai_message.update!(
          content: "Failed to generate image. Please try again."
        )
      end
    end
  end
  ```

### Phase 7: Testing

- [ ] Model tests for Chat
  ```ruby
  # test/models/chat_test.rb
  class ChatTest < ActiveSupport::TestCase
    setup do
      @account = accounts(:one)
      @user = users(:one)
      @chat = Chat.create!(
        account: @account,
        model_id: 'openrouter/auto',
        title: 'Test Chat'
      )
    end
    
    test "should belong to account" do
      assert_equal @account, @chat.account
    end
    
    test "should have many messages" do
      message = @chat.messages.create!(
        user: @user,
        role: 'user',
        content: 'Hello'
      )
      assert_includes @chat.messages, message
    end
    
    test "send_user_message creates message and queues job" do
      assert_difference '@chat.messages.count' do
        assert_enqueued_with(job: AiResponseJob) do
          @chat.send_user_message(
            user: @user,
            content: 'Test message'
          )
        end
      end
    end
    
    test "broadcasts to account on update" do
      assert_broadcast_on("Account:#{@account.obfuscated_id}") do
        @chat.update!(title: 'New Title')
      end
    end
  end
  ```

- [ ] Model tests for Message
  ```ruby
  # test/models/message_test.rb
  class MessageTest < ActiveSupport::TestCase
    setup do
      @chat = chats(:one)
      @user = users(:one)
    end
    
    test "user messages require user" do
      message = @chat.messages.build(role: 'user', content: 'Test')
      assert_not message.valid?
      assert_includes message.errors[:user], "is required for user messages"
    end
    
    test "assistant messages cannot have user" do
      message = @chat.messages.build(
        role: 'assistant',
        content: 'Test',
        user: @user
      )
      assert_not message.valid?
      assert_includes message.errors[:user], "must be blank for AI messages"
    end
    
    test "append_content debounces updates" do
      message = @chat.messages.create!(
        role: 'assistant',
        content: ''
      )
      
      # Small update shouldn't save
      message.append_content!('Hello')
      assert_equal '', message.reload.content
      
      # Large update should save
      message.append_content!(' ' * 100)
      assert message.reload.content.length > 0
    end
    
    test "broadcasts to chat on update" do
      message = @chat.messages.create!(
        user: @user,
        role: 'user',
        content: 'Test'
      )
      
      assert_broadcast_on("Chat:#{@chat.obfuscated_id}") do
        message.update!(content: 'Updated')
      end
    end
  end
  ```

- [ ] Controller tests
  ```ruby
  # test/controllers/chats_controller_test.rb
  class ChatsControllerTest < ActionDispatch::IntegrationTest
    setup do
      @user = users(:one)
      @account = @user.accounts.first
      sign_in @user
    end
    
    test "should get index" do
      get account_chats_path(@account)
      assert_response :success
    end
    
    test "should create chat" do
      assert_difference 'Chat.count' do
        post account_chats_path(@account), params: {
          chat: { title: 'New Chat', model_id: 'openrouter/auto' }
        }
      end
      assert_redirected_to account_chat_path(@account, Chat.last)
    end
    
    test "should show chat" do
      chat = @account.chats.create!(model_id: 'openrouter/auto')
      get account_chat_path(@account, chat)
      assert_response :success
    end
    
    test "should destroy chat" do
      chat = @account.chats.create!(model_id: 'openrouter/auto')
      assert_difference 'Chat.count', -1 do
        delete account_chat_path(@account, chat)
      end
      assert_redirected_to account_chats_path(@account)
    end
  end
  ```

## Key Design Decisions

1. **Using RubyLLM's acts_as methods** - Leverages the gem's built-in Rails integration for cleaner code
2. **Broadcastable for sync** - Reuses existing synchronization system instead of creating new channels
3. **Debounced streaming** - Prevents overwhelming the browser with too many updates
4. **Background job streaming** - More reliable than in-request streaming, handles failures gracefully
5. **Account-scoped chats** - Enables team collaboration on AI conversations
6. **OpenRouter by default** - Provides access to 500+ models with automatic fallback
7. **No billing logic** - Focused on core functionality, billing can be added later if needed

## Testing Strategy

1. **Mock RubyLLM in tests** - Use stubs to avoid API calls in test suite
2. **Test broadcasting** - Verify sync messages are sent correctly
3. **Test job retry logic** - Ensure rate limits are handled gracefully
4. **Integration tests** - Full flow from message creation to AI response

## Security Considerations

1. **API keys in credentials** - Never commit keys to version control
2. **Account scoping** - Users can only access chats in their accounts
3. **File upload validation** - ActiveStorage handles file type/size validation
4. **Content filtering** - RubyLLM handles content policy violations

## Performance Optimizations

1. **Streaming with debouncing** - Reduces database writes and broadcasts
2. **Background processing** - Keeps web requests fast
3. **Includes for N+1 prevention** - Eager load associations
4. **Model caching** - Cache available models list

## Migration from Previous Spec

The previous specification used `ruby-openai` gem directly. This spec uses `ruby_llm` which provides:
- Unified API across 500+ models
- Built-in Rails integration with `acts_as_*` methods
- Automatic message persistence
- Better error handling and retry logic
- Multi-provider support (OpenRouter, OpenAI, Anthropic, etc.)

## Notes

- RubyLLM requires Rails 7.0+ (we have Rails 8, so we're good)
- The gem automatically handles file attachments through ActiveStorage
- Streaming is built into the gem with block syntax
- Model registry is populated from the gem's database