# Ruby LLM Integration - Simplified Specification

## Executive Summary

This specification outlines a straightforward Rails implementation for AI conversations using the ruby-openai gem. The design prioritizes simplicity: two database tables, fat models with AI logic, and direct ActionCable streaming. No unnecessary abstractions, no premature optimizations, just working code that's easy to understand and maintain.

## Core Requirements

1. **Multi-user conversations** - Multiple users from the same account can participate
2. **Conversation history** - Full history stored and accessible
3. **Model selection** - Each conversation picks its LLM model
4. **Token tracking** - Simple usage tracking for billing
5. **File attachments** - Support documents, images, and audio via Active Storage
6. **Real-time streaming** - Stream AI responses via ActionCable

## Database Design (2 Tables Only)

### conversations table
```ruby
create_table :conversations do |t|
  t.references :account, null: false, foreign_key: true
  t.string :title
  t.string :model, null: false, default: "gpt-4o"
  t.integer :total_tokens, default: 0
  t.decimal :total_cost, precision: 10, scale: 4, default: 0.0
  t.timestamps
end

add_index :conversations, [:account_id, :created_at]
```

### messages table
```ruby
create_table :messages do |t|
  t.references :conversation, null: false, foreign_key: true
  t.references :user, foreign_key: true # null for AI messages
  t.string :role, null: false # "user", "assistant", "system"
  t.text :content
  t.integer :tokens
  t.decimal :cost, precision: 10, scale: 4
  t.string :model # Track which model version was used
  t.timestamps
end

add_index :messages, [:conversation_id, :created_at]
```

## Models (Fat and Explicit)

### app/models/conversation.rb
```ruby
class Conversation < ApplicationRecord
  belongs_to :account
  has_many :messages, dependent: :destroy
  has_many :participants, -> { distinct }, through: :messages, source: :user
  
  validates :model, presence: true, inclusion: { 
    in: %w[gpt-4o gpt-4o-mini claude-3-5-sonnet anthropic/claude-3-opus] 
  }
  
  # Generate title from first message if not provided
  after_create :generate_title_later, unless: :title?
  
  # Available models with their pricing (per 1K tokens)
  MODELS = {
    "gpt-4o" => { input: 0.0025, output: 0.01, provider: :openai },
    "gpt-4o-mini" => { input: 0.00015, output: 0.0006, provider: :openai },
    "claude-3-5-sonnet" => { input: 0.003, output: 0.015, provider: :anthropic }
  }.freeze
  
  def send_message(user:, content:, attachments: [])
    message = messages.create!(
      user: user,
      role: "user",
      content: content
    )
    
    # Attach files if provided
    attachments.each { |file| message.files.attach(file) }
    
    # Generate AI response
    generate_ai_response(message)
  end
  
  def generate_ai_response(user_message)
    # Build conversation context
    context = messages.order(:created_at).map do |msg|
      { role: msg.role, content: msg.content_with_attachments }
    end
    
    # Create placeholder for streaming
    ai_message = messages.create!(
      role: "assistant",
      content: "",
      model: model
    )
    
    # Stream response via ActionCable
    stream_ai_response(ai_message, context)
    
    ai_message
  end
  
  private
  
  def stream_ai_response(ai_message, context)
    client = ai_client_for(model)
    full_response = ""
    
    client.chat(
      parameters: {
        model: model_name_for_provider,
        messages: context,
        stream: proc do |chunk|
          if text = extract_text_from_chunk(chunk)
            full_response += text
            
            # Broadcast each chunk via ActionCable
            ActionCable.server.broadcast(
              "conversation_#{id}",
              {
                type: "chunk",
                message_id: ai_message.id,
                content: text
              }
            )
          end
        end
      }
    )
    
    # Update message with final content and token count
    tokens = estimate_tokens(full_response)
    cost = calculate_cost(tokens, :output)
    
    ai_message.update!(
      content: full_response,
      tokens: tokens,
      cost: cost
    )
    
    # Update conversation totals
    increment!(:total_tokens, tokens)
    increment!(:total_cost, cost)
    
    # Broadcast completion
    ActionCable.server.broadcast(
      "conversation_#{id}",
      {
        type: "complete",
        message_id: ai_message.id,
        tokens: tokens,
        cost: cost
      }
    )
  rescue => e
    Rails.logger.error "AI Response Error: #{e.message}"
    ai_message.update!(content: "Error: #{e.message}")
    
    ActionCable.server.broadcast(
      "conversation_#{id}",
      {
        type: "error",
        message_id: ai_message.id,
        error: e.message
      }
    )
  end
  
  def ai_client_for(model_key)
    provider = MODELS[model_key][:provider]
    
    case provider
    when :openai
      @openai_client ||= OpenAI::Client.new(
        access_token: Rails.application.credentials.openai_api_key
      )
    when :anthropic
      # Add Anthropic client when needed
      raise "Anthropic not yet implemented"
    end
  end
  
  def model_name_for_provider
    # Map our model names to provider-specific names if needed
    case model
    when "gpt-4o", "gpt-4o-mini"
      model
    when "claude-3-5-sonnet"
      "claude-3-5-sonnet-20241022"
    else
      model
    end
  end
  
  def extract_text_from_chunk(chunk)
    # OpenAI streaming format
    chunk.dig("choices", 0, "delta", "content")
  end
  
  def estimate_tokens(text)
    # Simple estimation: ~4 characters per token
    # For production, use tiktoken gem
    (text.length / 4.0).ceil
  end
  
  def calculate_cost(tokens, type)
    price_per_1k = MODELS[model][type]
    (tokens / 1000.0 * price_per_1k).round(4)
  end
  
  def generate_title_later
    # Simple: Use first 50 chars of first message
    # Could enhance with AI-generated title if desired
    return unless first_message = messages.first
    
    title = first_message.content.truncate(50)
    update_column(:title, title)
  end
end
```

### app/models/message.rb
```ruby
class Message < ApplicationRecord
  belongs_to :conversation
  belongs_to :user, optional: true # AI messages have no user
  
  has_many_attached :files
  
  validates :role, presence: true, inclusion: { in: %w[user assistant system] }
  validates :content, presence: true
  validate :user_required_for_user_role
  
  scope :from_users, -> { where(role: "user") }
  scope :from_ai, -> { where(role: "assistant") }
  
  # Format content with attachment references for AI context
  def content_with_attachments
    return content unless files.attached?
    
    attachment_texts = files.map do |file|
      case file.content_type
      when /image/
        "[Image: #{file.filename}]"
      when /audio/
        "[Audio: #{file.filename}]"
      when /pdf/, /text/
        # For text files, we could extract content
        "[Document: #{file.filename}]"
      else
        "[File: #{file.filename}]"
      end
    end
    
    "#{content}\n\nAttachments: #{attachment_texts.join(', ')}"
  end
  
  private
  
  def user_required_for_user_role
    if role == "user" && user_id.blank?
      errors.add(:user, "is required for user messages")
    elsif role != "user" && user_id.present?
      errors.add(:user, "must be blank for AI messages")
    end
  end
end
```

## Controllers (Thin and RESTful)

### app/controllers/conversations_controller.rb
```ruby
class ConversationsController < ApplicationController
  before_action :set_conversation, only: [:show, :destroy]
  
  def index
    @conversations = current_account.conversations
                                   .includes(:messages)
                                   .order(updated_at: :desc)
    
    render inertia: "Conversations/Index", props: {
      conversations: @conversations.map { |c| conversation_json(c) }
    }
  end
  
  def show
    @messages = @conversation.messages
                            .includes(:user, files_attachments: :blob)
                            .order(:created_at)
    
    render inertia: "Conversations/Show", props: {
      conversation: conversation_json(@conversation),
      messages: @messages.map { |m| message_json(m) }
    }
  end
  
  def create
    @conversation = current_account.conversations.create!(conversation_params)
    
    redirect_to @conversation
  end
  
  def destroy
    @conversation.destroy
    redirect_to conversations_path
  end
  
  private
  
  def set_conversation
    @conversation = current_account.conversations.find(params[:id])
  end
  
  def conversation_params
    params.require(:conversation).permit(:title, :model)
  end
  
  def conversation_json(conversation)
    {
      id: conversation.id,
      title: conversation.title || "New Conversation",
      model: conversation.model,
      total_tokens: conversation.total_tokens,
      total_cost: conversation.total_cost,
      last_message_at: conversation.messages.maximum(:created_at),
      message_count: conversation.messages.count
    }
  end
  
  def message_json(message)
    {
      id: message.id,
      role: message.role,
      content: message.content,
      user: message.user && {
        id: message.user.id,
        name: message.user.full_name,
        avatar_url: message.user.avatar_url
      },
      files: message.files.map { |f| 
        {
          id: f.id,
          filename: f.filename.to_s,
          url: rails_blob_url(f)
        }
      },
      created_at: message.created_at
    }
  end
end
```

### app/controllers/messages_controller.rb
```ruby
class MessagesController < ApplicationController
  before_action :set_conversation
  
  def create
    @message = @conversation.send_message(
      user: current_user,
      content: params[:content],
      attachments: params[:files] || []
    )
    
    # Return immediately - AI response will stream via ActionCable
    render json: message_json(@message)
  end
  
  private
  
  def set_conversation
    @conversation = current_account.conversations.find(params[:conversation_id])
  end
  
  def message_json(message)
    {
      id: message.id,
      role: message.role,
      content: message.content,
      user: message.user && {
        id: message.user.id,
        name: message.user.full_name
      },
      created_at: message.created_at
    }
  end
end
```

## ActionCable Channel (Direct Streaming)

### app/channels/conversation_channel.rb
```ruby
class ConversationChannel < ApplicationCable::Channel
  def subscribed
    conversation = current_user.accounts
                              .joins(:conversations)
                              .find_by(conversations: { id: params[:id] })
    
    if conversation
      stream_from "conversation_#{params[:id]}"
    else
      reject
    end
  end
end
```

## Frontend Components (Svelte 5)

### app/frontend/pages/Conversations/Show.svelte
```svelte
<script>
  import { onMount, onDestroy } from 'svelte';
  import { page } from '@inertiajs/svelte';
  import MessageList from '../../components/MessageList.svelte';
  import MessageInput from '../../components/MessageInput.svelte';
  
  let { conversation, messages: initialMessages } = $props();
  let messages = $state(initialMessages);
  let cable = $state(null);
  let subscription = $state(null);
  
  onMount(() => {
    // Setup ActionCable subscription
    cable = window.App.cable;
    subscription = cable.subscriptions.create(
      { 
        channel: "ConversationChannel", 
        id: conversation.id 
      },
      {
        received(data) {
          handleCableMessage(data);
        }
      }
    );
  });
  
  onDestroy(() => {
    if (subscription) {
      subscription.unsubscribe();
    }
  });
  
  function handleCableMessage(data) {
    switch(data.type) {
      case 'chunk':
        // Update message content as it streams
        const message = messages.find(m => m.id === data.message_id);
        if (message) {
          message.content += data.content;
        }
        break;
        
      case 'complete':
        // Update token count and cost when complete
        const completeMsg = messages.find(m => m.id === data.message_id);
        if (completeMsg) {
          completeMsg.tokens = data.tokens;
          completeMsg.cost = data.cost;
        }
        break;
        
      case 'error':
        console.error('AI Error:', data.error);
        break;
    }
  }
  
  async function sendMessage(content, files) {
    const formData = new FormData();
    formData.append('content', content);
    files.forEach(file => formData.append('files[]', file));
    
    const response = await fetch(`/conversations/${conversation.id}/messages`, {
      method: 'POST',
      headers: {
        'X-CSRF-Token': document.querySelector('[name="csrf-token"]').content
      },
      body: formData
    });
    
    const userMessage = await response.json();
    messages = [...messages, userMessage];
    
    // Add placeholder for AI response that will stream in
    messages = [...messages, {
      id: Date.now(), // Temporary ID
      role: 'assistant',
      content: '',
      created_at: new Date()
    }];
  }
</script>

<div class="flex flex-col h-screen">
  <header class="p-4 border-b">
    <h1 class="text-xl font-semibold">
      {conversation.title || 'New Conversation'}
    </h1>
    <div class="text-sm text-gray-500">
      Model: {conversation.model} • 
      Tokens: {conversation.total_tokens} • 
      Cost: ${conversation.total_cost.toFixed(4)}
    </div>
  </header>
  
  <MessageList {messages} />
  
  <MessageInput onSend={sendMessage} />
</div>
```

## Routes

```ruby
# config/routes.rb
resources :conversations do
  resources :messages, only: [:create]
end
```

## Migration

```ruby
class CreateConversationsAndMessages < ActiveRecord::Migration[7.1]
  def change
    create_table :conversations do |t|
      t.references :account, null: false, foreign_key: true
      t.string :title
      t.string :model, null: false, default: "gpt-4o"
      t.integer :total_tokens, default: 0
      t.decimal :total_cost, precision: 10, scale: 4, default: 0.0
      t.timestamps
    end
    
    add_index :conversations, [:account_id, :created_at]
    
    create_table :messages do |t|
      t.references :conversation, null: false, foreign_key: true
      t.references :user, foreign_key: true
      t.string :role, null: false
      t.text :content
      t.integer :tokens
      t.decimal :cost, precision: 10, scale: 4
      t.string :model
      t.timestamps
    end
    
    add_index :messages, [:conversation_id, :created_at]
  end
end
```

## Implementation Checklist

### Phase 1: Database and Models
- [ ] Create migration for conversations and messages tables
- [ ] Implement Conversation model with AI integration
- [ ] Implement Message model with file attachments
- [ ] Add OpenAI credentials to Rails credentials

### Phase 2: Controllers and Routes
- [ ] Create ConversationsController
- [ ] Create MessagesController
- [ ] Add routes for conversations and messages
- [ ] Test basic CRUD operations

### Phase 3: ActionCable Streaming
- [ ] Create ConversationChannel
- [ ] Test streaming with OpenAI API
- [ ] Handle connection errors gracefully

### Phase 4: Frontend Components
- [ ] Create Conversation index page
- [ ] Create Conversation show page with chat interface
- [ ] Implement message input with file upload
- [ ] Add real-time streaming display

### Phase 5: Polish and Testing
- [ ] Add conversation title generation
- [ ] Implement token counting (tiktoken gem)
- [ ] Add system tests for full flow
- [ ] Add cost tracking display

## Key Differences from Original Spec

1. **Only 2 tables** instead of 5 - Everything fits in Conversations and Messages
2. **No service objects** - AI logic lives in the Conversation model
3. **No background jobs** - Streaming happens directly in request/response cycle
4. **Direct ActionCable** - Stream chunks directly, don't store in database
5. **Simple cost tracking** - Just track totals, not detailed breakdowns
6. **No abstractions** - No concerns, no acts_as_*, just explicit Ruby code
7. **RESTful controllers** - Thin controllers that delegate to fat models

## Notes

- The `ruby-openai` gem is already in the Gemfile
- Active Storage is configured for file attachments
- ActionCable uses the existing session-based authentication
- No premature optimization - add caching/background jobs only if needed
- Token counting is simplified - use tiktoken gem for accuracy in production
- Cost calculations use 4 decimal places (sufficient for even small amounts)

This approach results in ~300 lines of application code instead of 1000+, while maintaining all functionality. The code is boring, predictable, and easy to understand - exactly what we want for maintainable software.