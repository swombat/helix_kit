# Ruby LLM Integration - Technical Specification

## Executive Summary

This specification details the integration of Ruby LLM into the Rails 8 + Svelte 5 application to provide AI-powered conversations with multi-modal support, real-time streaming responses, and comprehensive usage tracking. The implementation follows Rails conventions strictly, leveraging fat models, thin controllers, and ActionCable for real-time updates.

## Architecture Overview

### Core Components
- **Conversation Model**: Account-scoped conversations with model selection
- **Message Model**: User and assistant messages with file attachments
- **MessageChunk Model**: Streaming response chunks for real-time updates
- **Usage Model**: Token tracking per conversation and model
- **ActionCable**: Real-time streaming via Broadcastable concern
- **ActiveStorage**: Document and image attachments via S3
- **Background Jobs**: AI response processing via SolidQueue

### Data Flow
1. User creates/selects conversation in Svelte UI
2. User sends message with optional attachments
3. Rails controller creates Message and enqueues AiResponseJob
4. Job streams response chunks via ActionCable
5. Svelte component receives real-time updates via useSync
6. Final message and usage stats persisted to database

## Database Schema

### Conversations Table
```ruby
create_table :conversations do |t|
  t.references :account, null: false, foreign_key: true
  t.references :created_by, null: false, foreign_key: { to_table: :users }
  t.string :title
  t.string :model_id, null: false
  t.string :system_prompt, limit: 5000
  t.decimal :temperature, precision: 3, scale: 2, default: 0.7
  t.integer :max_tokens, default: 1000
  t.integer :messages_count, default: 0
  t.boolean :active, default: true
  t.jsonb :metadata, default: {}
  t.timestamps
  
  t.index [:account_id, :created_at]
  t.index [:account_id, :active]
end
```

### Messages Table
```ruby
create_table :messages do |t|
  t.references :conversation, null: false, foreign_key: true
  t.references :user, foreign_key: true # null for assistant messages
  t.string :role, null: false # 'user', 'assistant', 'system'
  t.text :content
  t.jsonb :metadata, default: {} # For tool calls, image URLs, etc.
  t.boolean :streaming, default: false
  t.datetime :completed_at
  t.timestamps
  
  t.index [:conversation_id, :created_at]
  t.index [:conversation_id, :role]
end
```

### Message Chunks Table (for streaming)
```ruby
create_table :message_chunks do |t|
  t.references :message, null: false, foreign_key: true
  t.integer :sequence, null: false
  t.text :content
  t.datetime :created_at, null: false
  
  t.index [:message_id, :sequence], unique: true
end
```

### Usages Table
```ruby
create_table :usages do |t|
  t.references :conversation, null: false, foreign_key: true
  t.references :message, null: false, foreign_key: true
  t.string :model_id, null: false
  t.integer :prompt_tokens, default: 0
  t.integer :completion_tokens, default: 0
  t.integer :total_tokens, default: 0
  t.decimal :cost, precision: 10, scale: 6 # Optional cost tracking
  t.jsonb :metadata, default: {} # Additional provider-specific data
  t.timestamps
  
  t.index [:conversation_id, :created_at]
  t.index :model_id
end
```

### Models Table (cached model metadata)
```ruby
create_table :models do |t|
  t.string :model_id, null: false, index: { unique: true }
  t.string :provider, null: false # 'openrouter', 'openai', etc.
  t.string :name, null: false
  t.string :description
  t.integer :context_length
  t.decimal :input_cost_per_1k, precision: 10, scale: 6
  t.decimal :output_cost_per_1k, precision: 10, scale: 6
  t.jsonb :capabilities, default: {} # vision, audio, tools, etc.
  t.boolean :available, default: true
  t.timestamps
  
  t.index :provider
  t.index :available
end
```

## Rails Models

### Conversation Model
```ruby
class Conversation < ApplicationRecord
  include SyncAuthorizable
  include Broadcastable
  
  # Ruby LLM integration
  acts_as_chat
  
  # Associations
  belongs_to :account
  belongs_to :created_by, class_name: "User"
  has_many :messages, dependent: :destroy
  has_many :usages, dependent: :destroy
  
  # Broadcasting
  broadcasts_to :account
  
  # Validations
  validates :title, presence: true
  validates :model_id, presence: true
  validates :temperature, numericality: { in: 0..2 }
  validates :max_tokens, numericality: { greater_than: 0, less_than_or_equal_to: 100000 }
  validate :model_exists_and_available
  
  # Callbacks
  before_validation :set_default_title, on: :create
  after_create :create_initial_system_message, if: :system_prompt?
  
  # Scopes
  scope :active, -> { where(active: true) }
  scope :for_account, ->(account) { where(account: account) }
  
  # Business Logic
  def send_message(content:, user:, attachments: [])
    transaction do
      # Create user message
      message = messages.create!(
        user: user,
        role: 'user',
        content: content
      )
      
      # Attach files if any
      attachments.each do |attachment|
        message.files.attach(attachment)
      end
      
      # Queue AI response
      AiResponseJob.perform_later(self, message)
      
      message
    end
  end
  
  def assistant_reply!(user_message)
    # Create assistant message
    assistant_message = messages.create!(
      role: 'assistant',
      streaming: true
    )
    
    # Stream response with chunks
    ask(user_message.content_with_attachments) do |chunk|
      # Create chunk record
      chunk_record = assistant_message.chunks.create!(
        sequence: assistant_message.chunks.count,
        content: chunk.content
      )
      
      # Broadcast chunk
      broadcast_chunk(assistant_message, chunk_record)
      
      # Update message content (debounced)
      assistant_message.append_chunk!(chunk.content)
    end
    
    # Finalize message
    assistant_message.finalize!
    
    # Track usage
    track_usage(assistant_message, response.usage)
    
    assistant_message
  end
  
  def total_tokens_used
    usages.sum(:total_tokens)
  end
  
  def estimated_cost
    usages.sum(:cost)
  end
  
  def can_continue?
    active? && messages.count < 1000 # Reasonable limit
  end
  
  private
  
  def set_default_title
    self.title ||= "New Conversation"
  end
  
  def create_initial_system_message
    messages.create!(
      role: 'system',
      content: system_prompt
    )
  end
  
  def model_exists_and_available
    model = Model.find_by(model_id: model_id)
    errors.add(:model_id, "is not available") unless model&.available?
  end
  
  def broadcast_chunk(message, chunk)
    ActionCable.server.broadcast(
      "Conversation:#{obfuscated_id}",
      {
        action: 'chunk',
        message_id: message.obfuscated_id,
        chunk: {
          sequence: chunk.sequence,
          content: chunk.content
        }
      }
    )
  end
  
  def track_usage(message, usage_data)
    usages.create!(
      message: message,
      model_id: model_id,
      prompt_tokens: usage_data.prompt_tokens,
      completion_tokens: usage_data.completion_tokens,
      total_tokens: usage_data.total_tokens,
      cost: calculate_cost(usage_data)
    )
  end
  
  def calculate_cost(usage_data)
    model = Model.find_by(model_id: model_id)
    return 0 unless model
    
    input_cost = (usage_data.prompt_tokens / 1000.0) * model.input_cost_per_1k
    output_cost = (usage_data.completion_tokens / 1000.0) * model.output_cost_per_1k
    input_cost + output_cost
  end
end
```

### Message Model
```ruby
class Message < ApplicationRecord
  include Broadcastable
  
  # Ruby LLM integration
  acts_as_message
  
  # File attachments
  has_many_attached :files do |attachable|
    attachable.variant :thumb, resize_to_limit: [200, 200]
    attachable.variant :preview, resize_to_limit: [800, 600]
  end
  
  # Associations
  belongs_to :conversation, counter_cache: true
  belongs_to :user, optional: true # null for assistant/system messages
  has_many :chunks, class_name: "MessageChunk", dependent: :destroy
  has_one :usage, dependent: :destroy
  
  # Broadcasting
  broadcasts_to :conversation
  
  # Validations
  validates :role, presence: true, inclusion: { in: %w[user assistant system] }
  validates :content, presence: true, unless: :streaming?
  validates :user, presence: true, if: -> { role == 'user' }
  validate :validate_file_types
  validate :validate_file_sizes
  
  # Scopes
  scope :from_users, -> { where(role: 'user') }
  scope :from_assistant, -> { where(role: 'assistant') }
  scope :completed, -> { where.not(completed_at: nil) }
  
  # For chunked streaming updates
  attr_accessor :chunk_buffer
  attr_accessor :last_broadcast_at
  
  def append_chunk!(chunk_content)
    self.chunk_buffer ||= ""
    self.chunk_buffer += chunk_content
    
    # Debounced update (500ms)
    if should_broadcast?
      self.content = chunk_buffer
      self.last_broadcast_at = Time.current
      save!(validate: false)
    end
  end
  
  def finalize!
    update!(
      content: chunk_buffer || content,
      streaming: false,
      completed_at: Time.current
    )
  end
  
  def content_with_attachments
    return content unless files.attached?
    
    # Build message with file references for Ruby LLM
    content_parts = [content]
    
    files.each do |file|
      if file.image?
        content_parts << "[Image: #{file.filename}]"
      elsif file.content_type.include?('pdf')
        content_parts << "[PDF: #{file.filename}]"
      else
        content_parts << "[File: #{file.filename}]"
      end
    end
    
    content_parts.join("\n")
  end
  
  def author_name
    user ? user.full_name : 'Assistant'
  end
  
  def author_avatar
    user&.avatar_url
  end
  
  private
  
  def should_broadcast?
    return true unless last_broadcast_at
    (Time.current - last_broadcast_at) > 0.5.seconds
  end
  
  def validate_file_types
    return unless files.attached?
    
    allowed_types = %w[
      image/png image/jpeg image/gif image/webp
      application/pdf text/plain text/markdown
      audio/mpeg audio/wav audio/m4a
    ]
    
    files.each do |file|
      unless allowed_types.include?(file.content_type)
        errors.add(:files, "#{file.filename} has unsupported type")
      end
    end
  end
  
  def validate_file_sizes
    return unless files.attached?
    
    max_size = 10.megabytes
    files.each do |file|
      if file.byte_size > max_size
        errors.add(:files, "#{file.filename} is too large (max 10MB)")
      end
    end
  end
end
```

### MessageChunk Model
```ruby
class MessageChunk < ApplicationRecord
  belongs_to :message
  
  validates :sequence, presence: true, uniqueness: { scope: :message_id }
  validates :content, presence: true
end
```

### Usage Model
```ruby
class Usage < ApplicationRecord
  belongs_to :conversation
  belongs_to :message
  
  validates :model_id, presence: true
  validates :prompt_tokens, numericality: { greater_than_or_equal_to: 0 }
  validates :completion_tokens, numericality: { greater_than_or_equal_to: 0 }
  validates :total_tokens, numericality: { greater_than: 0 }
  
  # Scopes for reporting
  scope :for_period, ->(start_date, end_date) {
    where(created_at: start_date..end_date)
  }
  scope :by_model, ->(model_id) { where(model_id: model_id) }
  
  def self.total_cost_for_account(account, period = nil)
    query = joins(:conversation).where(conversations: { account: account })
    query = query.for_period(*period) if period
    query.sum(:cost)
  end
end
```

### Model Model (cached provider data)
```ruby
class Model < ApplicationRecord
  acts_as_model # Ruby LLM integration
  
  validates :model_id, presence: true, uniqueness: true
  validates :provider, presence: true
  validates :name, presence: true
  
  scope :available, -> { where(available: true) }
  scope :with_vision, -> { where("capabilities->>'vision' = 'true'") }
  scope :with_audio, -> { where("capabilities->>'audio' = 'true'") }
  scope :for_provider, ->(provider) { where(provider: provider) }
  
  def supports_vision?
    capabilities['vision'] == true
  end
  
  def supports_audio?
    capabilities['audio'] == true
  end
  
  def supports_tools?
    capabilities['tools'] == true
  end
  
  def display_name
    "#{name} (#{provider})"
  end
  
  # Class method to refresh from Ruby LLM
  def self.refresh_from_providers!
    # This would be called by a rake task or job
    # Implementation depends on Ruby LLM's model listing API
  end
end
```

## Controllers

### ConversationsController
```ruby
class ConversationsController < ApplicationController
  before_action :set_account
  before_action :set_conversation, only: [:show, :destroy]
  
  def index
    @conversations = @account.conversations.active
                             .includes(:created_by, :messages)
                             .order(updated_at: :desc)
    
    render inertia: 'Conversations/Index', props: {
      account: @account,
      conversations: @conversations.map { |c| 
        c.as_json(
          only: [:id, :title, :model_id, :created_at, :updated_at],
          methods: [:messages_count],
          include: {
            created_by: { only: [:id], methods: [:full_name] },
            last_message: { only: [:content, :role, :created_at] }
          }
        )
      },
      available_models: available_models
    }
  end
  
  def show
    @messages = @conversation.messages
                            .includes(:user, :chunks, files_attachments: :blob)
                            .order(:created_at)
    
    render inertia: 'Conversations/Show', props: {
      account: @account,
      conversation: @conversation.as_json(
        only: [:id, :title, :model_id, :system_prompt, :temperature, :max_tokens],
        methods: [:total_tokens_used, :estimated_cost]
      ),
      messages: @messages.map { |m|
        m.as_json(
          only: [:id, :role, :content, :created_at, :streaming, :completed_at],
          methods: [:author_name, :author_avatar],
          include: {
            files: { 
              only: [:id, :filename, :content_type, :byte_size],
              methods: [:url, :preview_url]
            }
          }
        )
      },
      available_models: available_models,
      current_user: current_user.as_json(only: [:id], methods: [:full_name])
    }
  end
  
  def create
    @conversation = @account.conversations.build(conversation_params)
    @conversation.created_by = current_user
    
    if @conversation.save
      redirect_to account_conversation_path(@account, @conversation)
    else
      redirect_back fallback_location: account_conversations_path(@account),
                   inertia: { errors: @conversation.errors }
    end
  end
  
  def destroy
    @conversation.update!(active: false)
    redirect_to account_conversations_path(@account), 
                notice: "Conversation archived"
  end
  
  private
  
  def set_account
    @account = current_user.accounts.find(params[:account_id])
  end
  
  def set_conversation
    @conversation = @account.conversations.find(params[:id])
  end
  
  def conversation_params
    params.require(:conversation).permit(
      :title, :model_id, :system_prompt, :temperature, :max_tokens
    )
  end
  
  def available_models
    Model.available.order(:name).map { |m|
      {
        id: m.model_id,
        name: m.display_name,
        context_length: m.context_length,
        supports_vision: m.supports_vision?,
        supports_audio: m.supports_audio?
      }
    }
  end
end
```

### MessagesController
```ruby
class MessagesController < ApplicationController
  before_action :set_conversation
  
  def create
    @message = @conversation.send_message(
      content: message_params[:content],
      user: current_user,
      attachments: message_params[:files]
    )
    
    if @message.persisted?
      # Response will stream via ActionCable
      head :ok
    else
      render json: { errors: @message.errors }, status: :unprocessable_entity
    end
  end
  
  private
  
  def set_conversation
    @account = current_user.accounts.find(params[:account_id])
    @conversation = @account.conversations.find(params[:conversation_id])
  end
  
  def message_params
    params.require(:message).permit(:content, files: [])
  end
end
```

## Background Jobs

### AiResponseJob
```ruby
class AiResponseJob < ApplicationJob
  queue_as :ai_processing
  
  def perform(conversation, user_message)
    # Create assistant message placeholder
    assistant_message = conversation.messages.create!(
      role: 'assistant',
      streaming: true,
      content: ''
    )
    
    begin
      # Configure Ruby LLM with conversation settings
      chat = RubyLLM.chat(
        model: conversation.model_id,
        temperature: conversation.temperature,
        max_tokens: conversation.max_tokens,
        system_prompt: conversation.system_prompt
      )
      
      # Stream response
      chunk_sequence = 0
      full_content = ''
      
      response = chat.ask(user_message.content) do |chunk|
        # Store chunk
        message_chunk = assistant_message.chunks.create!(
          sequence: chunk_sequence,
          content: chunk.content
        )
        chunk_sequence += 1
        
        # Accumulate content
        full_content += chunk.content
        
        # Broadcast via ActionCable (debounced in Message model)
        assistant_message.append_chunk!(chunk.content)
      end
      
      # Finalize message
      assistant_message.finalize!
      
      # Track usage
      if response.usage
        Usage.create!(
          conversation: conversation,
          message: assistant_message,
          model_id: conversation.model_id,
          prompt_tokens: response.usage.prompt_tokens,
          completion_tokens: response.usage.completion_tokens,
          total_tokens: response.usage.total_tokens,
          cost: calculate_cost(conversation.model_id, response.usage)
        )
      end
      
    rescue RubyLLM::APIError => e
      handle_api_error(assistant_message, e)
    rescue RubyLLM::RateLimitError => e
      # Retry with exponential backoff
      retry_job wait: calculate_backoff
    end
  end
  
  private
  
  def calculate_cost(model_id, usage)
    model = Model.find_by(model_id: model_id)
    return 0 unless model
    
    input_cost = (usage.prompt_tokens / 1000.0) * (model.input_cost_per_1k || 0)
    output_cost = (usage.completion_tokens / 1000.0) * (model.output_cost_per_1k || 0)
    input_cost + output_cost
  end
  
  def handle_api_error(message, error)
    message.update!(
      content: "Error: #{error.message}",
      streaming: false,
      completed_at: Time.current,
      metadata: { error: error.class.name, message: error.message }
    )
  end
  
  def calculate_backoff
    (2 ** (executions - 1)).seconds
  end
end
```

## Real-time Synchronization

### ActionCable Channel Updates
The existing Broadcastable concern handles most updates automatically. For streaming chunks, we need custom broadcasting:

```ruby
# In Message model
def broadcast_streaming_chunk(chunk)
  ActionCable.server.broadcast(
    "Conversation:#{conversation.obfuscated_id}",
    {
      action: 'streaming_chunk',
      message_id: obfuscated_id,
      chunk: {
        content: chunk.content,
        sequence: chunk.sequence
      }
    }
  )
end
```

### Svelte Integration
```svelte
<!-- Conversations/Show.svelte -->
<script>
  import { useSync, createDynamicSync } from '$lib/use-sync';
  import { onMount } from 'svelte';
  
  let { account, conversation, messages = [], available_models } = $props();
  
  // Subscribe to conversation updates
  useSync({
    [`Conversation:${conversation.id}`]: ['conversation', 'messages']
  });
  
  // Handle streaming chunks separately
  onMount(() => {
    const channel = `Conversation:${conversation.id}`;
    
    // Custom handler for streaming chunks
    const handleStreamingChunk = (data) => {
      if (data.action === 'streaming_chunk') {
        const message = messages.find(m => m.id === data.message_id);
        if (message) {
          message.content += data.chunk.content;
          message.streaming = true;
        }
      }
    };
    
    // Subscribe to custom events
    // Implementation depends on ActionCable client setup
  });
</script>
```

## File Upload Configuration

### Update storage.yml for S3
```yaml
amazon:
  service: S3
  access_key_id: <%= Rails.application.credentials.dig(:aws, :access_key_id) %>
  secret_access_key: <%= Rails.application.credentials.dig(:aws, :secret_access_key) %>
  region: <%= Rails.application.credentials.dig(:aws, :region) || 'us-east-1' %>
  bucket: <%= Rails.application.credentials.dig(:aws, :bucket) %>-<%= Rails.env %>
```

### Update environments
```ruby
# config/environments/production.rb
config.active_storage.service = :amazon

# config/environments/development.rb  
config.active_storage.service = :local # or :amazon for testing
```

## Routes Configuration

```ruby
# config/routes.rb
resources :accounts do
  resources :conversations do
    resources :messages, only: [:create]
  end
end
```

## Initializer Configuration

```ruby
# config/initializers/ruby_llm.rb
RubyLLM.configure do |config|
  config.openrouter_api_key = Rails.application.credentials.dig(:openrouter, :api_key)
  
  # Optional: other providers
  config.openai_api_key = Rails.application.credentials.dig(:openai, :api_key)
  config.anthropic_api_key = Rails.application.credentials.dig(:anthropic, :api_key)
end
```

## Migration Files

### Create Conversations
```ruby
class CreateConversations < ActiveRecord::Migration[8.0]
  def change
    create_table :conversations do |t|
      t.references :account, null: false, foreign_key: true
      t.references :created_by, null: false, foreign_key: { to_table: :users }
      t.string :title
      t.string :model_id, null: false
      t.string :system_prompt, limit: 5000
      t.decimal :temperature, precision: 3, scale: 2, default: 0.7
      t.integer :max_tokens, default: 1000
      t.integer :messages_count, default: 0
      t.boolean :active, default: true
      t.jsonb :metadata, default: {}
      t.timestamps
      
      t.index [:account_id, :created_at]
      t.index [:account_id, :active]
    end
  end
end
```

### Create Messages
```ruby
class CreateMessages < ActiveRecord::Migration[8.0]
  def change
    create_table :messages do |t|
      t.references :conversation, null: false, foreign_key: true
      t.references :user, foreign_key: true
      t.string :role, null: false
      t.text :content
      t.jsonb :metadata, default: {}
      t.boolean :streaming, default: false
      t.datetime :completed_at
      t.timestamps
      
      t.index [:conversation_id, :created_at]
      t.index [:conversation_id, :role]
    end
  end
end
```

### Create Message Chunks
```ruby
class CreateMessageChunks < ActiveRecord::Migration[8.0]
  def change
    create_table :message_chunks do |t|
      t.references :message, null: false, foreign_key: true
      t.integer :sequence, null: false
      t.text :content
      t.datetime :created_at, null: false
      
      t.index [:message_id, :sequence], unique: true
    end
  end
end
```

### Create Usages
```ruby
class CreateUsages < ActiveRecord::Migration[8.0]
  def change
    create_table :usages do |t|
      t.references :conversation, null: false, foreign_key: true
      t.references :message, null: false, foreign_key: true
      t.string :model_id, null: false
      t.integer :prompt_tokens, default: 0
      t.integer :completion_tokens, default: 0
      t.integer :total_tokens, default: 0
      t.decimal :cost, precision: 10, scale: 6
      t.jsonb :metadata, default: {}
      t.timestamps
      
      t.index [:conversation_id, :created_at]
      t.index :model_id
    end
  end
end
```

### Create Models
```ruby
class CreateModels < ActiveRecord::Migration[8.0]
  def change
    create_table :models do |t|
      t.string :model_id, null: false
      t.string :provider, null: false
      t.string :name, null: false
      t.string :description
      t.integer :context_length
      t.decimal :input_cost_per_1k, precision: 10, scale: 6
      t.decimal :output_cost_per_1k, precision: 10, scale: 6
      t.jsonb :capabilities, default: {}
      t.boolean :available, default: true
      t.timestamps
      
      t.index :model_id, unique: true
      t.index :provider
      t.index :available
    end
  end
end
```

## Implementation Checklist

### Phase 1: Foundation
- [ ] Install Ruby LLM gem and run generator
- [ ] Create migration files for all tables
- [ ] Run migrations
- [ ] Create Model, Conversation, Message, MessageChunk, Usage models
- [ ] Add acts_as declarations to models
- [ ] Configure Ruby LLM initializer with OpenRouter API key
- [ ] Run `rails ruby_llm:load_models` to populate models table

### Phase 2: Core Functionality
- [ ] Implement ConversationsController with index/show/create
- [ ] Implement MessagesController with create action
- [ ] Create AiResponseJob for background processing
- [ ] Add Broadcastable to Conversation and Message models
- [ ] Update routes.rb with nested resources
- [ ] Test basic conversation creation and message sending

### Phase 3: Real-time Streaming
- [ ] Implement streaming chunk handling in AiResponseJob
- [ ] Add custom ActionCable broadcasting for chunks
- [ ] Update Message model with append_chunk! and finalize! methods
- [ ] Create MessageChunk records during streaming
- [ ] Test real-time updates via ActionCable

### Phase 4: File Attachments
- [ ] Configure S3 storage in storage.yml
- [ ] Add AWS credentials to Rails credentials
- [ ] Update production environment to use S3
- [ ] Add file attachment validations to Message model
- [ ] Implement file handling in message creation
- [ ] Test image and document uploads

### Phase 5: Frontend Integration
- [ ] Create Conversations/Index.svelte component
- [ ] Create Conversations/Show.svelte component
- [ ] Implement useSync for real-time updates
- [ ] Add file upload UI component
- [ ] Create message input with attachment support
- [ ] Implement streaming message display

### Phase 6: Usage Tracking
- [ ] Implement Usage model with cost calculation
- [ ] Add usage tracking to AiResponseJob
- [ ] Create usage summary methods on Conversation
- [ ] Add usage display to conversation UI
- [ ] Optional: Create usage reports for accounts

### Phase 7: Polish
- [ ] Add model selection UI in conversation settings
- [ ] Implement conversation search/filtering
- [ ] Add conversation title auto-generation
- [ ] Implement error handling and retry logic
- [ ] Add loading states and progress indicators
- [ ] Test with multiple model providers

## Testing Strategy

### Model Tests
- Test Conversation#send_message with attachments
- Test Message#append_chunk! debouncing
- Test Usage cost calculations
- Test Model capability methods
- Test broadcasting behaviors

### Controller Tests
- Test conversation creation with different models
- Test message creation with files
- Test authorization (account scoping)
- Test error handling for invalid inputs

### Job Tests
- Test AiResponseJob with mock Ruby LLM responses
- Test streaming chunk creation
- Test error handling and retries
- Test usage tracking

### Integration Tests
- Test full conversation flow with real API (in development)
- Test file upload and processing
- Test real-time updates via ActionCable
- Test model switching mid-conversation

## Security Considerations

1. **API Key Security**: Store all API keys in Rails credentials
2. **File Upload Security**: Validate file types and sizes, scan for malware
3. **Authorization**: All conversations scoped to accounts via associations
4. **Rate Limiting**: Implement per-account rate limits for API calls
5. **Content Filtering**: Optional content moderation before sending to API
6. **Cost Control**: Set spending limits per account

## Performance Optimizations

1. **Streaming Debounce**: 500ms debounce for UI updates during streaming
2. **Background Processing**: All AI calls in background jobs
3. **Eager Loading**: Include associations to prevent N+1 queries
4. **Partial Reloads**: Use Inertia's `only` option for message updates
5. **File Processing**: Resize images before sending to API
6. **Caching**: Cache model metadata and capabilities

## Future Enhancements

1. **Tool Calling**: Add support for function calling when Ruby LLM supports it
2. **Conversation Templates**: Pre-configured system prompts and settings
3. **Collaborative Editing**: Multiple users editing same conversation
4. **Export/Import**: Download conversations as JSON/Markdown
5. **Advanced Search**: Full-text search across conversations
6. **Analytics Dashboard**: Usage trends and insights